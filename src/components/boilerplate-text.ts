export const boilerplateText = {
  // paragraphsnow: [
  //   "snowflake enables every organization to mobilize their data with snowflakes data cloud customers use the data cloud to unite siloed data discover and securely share data power data applications and execute diverse aiml and analytic workloads",
  //   "wherever data or users live snowflake delivers a single data experience that spans multiple clouds and geographies thousands of customers across many industries including 647 of the 2023 forbes global 2000 g2k use snowflake data cloud to power their businesses",
  //   "these applications are not only transforming the way we interact with technology but also reshaping various sectors of society theyre harnessing the power of vast amounts of data and leveraging advanced artificial intelligence ai algorithms",
  //   "in healthcare modern applications are being used to predict disease outbreaks improve patient care and accelerate drug discovery in education applications provide personalized learning experiences and early identification of students who need more help",
  //   "the snowflake data cloud is the best choice for application architecture because it simplifies development and operations by providing a unified secure and fully governed cloud environment for data storage integration analysis and other computing tasks",
  //   "snowpark was designed to make building data pipelines and aiml models in snowflake a breeze using programming languages such as python java and scala without any data movement",
  //   "modern apps are data intensive and ai enriched they process large volumes of complex and fast changing data from different sources analyze it with aimachine learning ml models",
  //   "in other words modern apps handle data collection processing and representation to provide value in the form of ai enriched insights here are some examples",
  //   "the chief advantage of three tier application architecture is that developers can develop modify and scale the tiers separately instead of changing the entire application",
  //   "at the bottom is the data tier sometimes called the database tier or persistence layer this is where the data that feeds the processing tier is stored and managed it includes the data storage and access mechanisms",
  //   "in the middle is the processing tier also known as the business logic or compute layer this is where business logic for the application is defined for example through a specific set of business rules",
  //   "this tier processes information thats collected in the presentation tier as well as information stored in the data tier through data transformations and ml models the processing tier can also add change or delete data in the data tier",
  //   "on the top is the presentation tier which houses the user interface this is where the user interacts with the data its main purpose is to display information communicate and collect data from the end user",
  //   "a cloud data platform provides a unified secure and fully governed cloud environment for data store integration analysis and other computing tasks",
  //   "snowpark is the set of libraries and runtimes in snowflake that securely enable developers to deploy and process non sql code including python java and scala",
  //   "on the server side runtimes include either python java and scala in the warehouse model or snowpark container services snowflakes virtual warehouses are compute clusters that host and run server side contracts",
  //   "for python developers snowparks python runtime makes it possible to write custom python code through udfs and stored procedures which are deployed into snowflakes secure python sandbox",
  //   "udfs and stored procedures are two key components of snowpark that allow developers to bring custom python logic to snowflakes compute engine while taking advantage of open source packages preinstalled from anaconda in snowpark",
  //   "custom logic written in python runs directly in snowflake using udfs these functions can stand alone or be called as part of a dataframe operation to process the data",
  //   "snowpark takes care of serializing the custom code into python byte code and pushes all of the logic to snowflake so it runs next to the data",
  //   "snowpark stored procedures help developers operationalize their python code and run orchestrate and schedule their pipelines",
  //   "a stored procedure is created once and can be executed many times with a simple call statement in your orchestration or automation tools snowflake supports stored procedures in sql python java javascript and scala",
  //   "to leverage open source innovation snowpark has partnered with anaconda for a product integration without any additional cost to the user beyond warehouse usage",
  //   "developers in snowflake are now able to speed up their python based pipelines by taking advantage of the seamless dependency management and comprehensive set of curated open source packages provided by anaconda",
  //   "snowpark optimized warehouses have compute nodes with 16x the memory and 10x the local cache compared with standard warehouses the larger memory helps unlock memory intensive use cases on large data sets",
  //   "snowpark container services is a new snowpark runtime option that enables developers to effortlessly deploy manage and scale containerized workloads jobs services service functions using secure snowflake managed infrastructure with configurable hardware options such as gpus",
  //   "with containers running in snowflake there is no need to move governed data outside of snowflake to use it as part of the most sophisticated aiml models and apps",
  //   "the containers built and packaged by developers using their tools of choice can include code in any programming language for example cc node js python r react etc",
  //   "snowflake native apps provide the building blocks for app development distribution operation and monetization all within snowflakes platform",
  //   "snowpark allows users to easily process and derive insights from unstructured data from files such as images videos and audio python developers can easily take advantage of the python ecosystem of open source packages",
  // ],
  // database: [
  //   "paxos is a family of protocols ensuring distributed agreement despite node failures it guarantees safety by requiring a majority quorum for proposals and commits making it robust but complex",
  //   "raft simplifies distributed consensus compared to paxos it uses leader election and replicated logs ensuring that all nodes agree on the sequence of operations in a fault tolerant manner",
  //   "bft mechanisms allow distributed systems to reach consensus even when some nodes exhibit arbitrary malicious behavior byzantine faults unlike protocols assuming only crash failures",
  //   "2pc coordinates atomic transactions across multiple distributed nodes it involves a prepare phase where nodes vote and a commitabort phase based on the vote outcome blocking is a risk",
  //   "3pc aims to reduce 2pcs blocking issue during coordinator or node failures it adds a pre commit phase allowing non blocking recovery under certain failure scenarios",
  //   "vector clocks track causality in distributed systems by assigning a vector timestamp to each event comparing vectors reveals if events are causally related concurrent or ordered",
  //   "matrix clocks extend vector clocks enabling nodes to maintain knowledge about other nodes knowledge of event times this helps track more complex causal relationships and global states",
  //   "lamport timestamps assign a logical clock value a simple counter to events establishing a total ordering consistent with causality concurrent events might get arbitrary order",
  //   "hlcs combine physical clock time with logical counters they provide timestamps that reflect causality like lamport clocks but stay close to physical time for better observability",
  //   "version vectors are used in replicated systems to track the history of updates for each replica they help detect and reconcile conflicting updates made concurrently",
  //   "crdts are data structures designed for replication where concurrent updates can merge automatically without conflicts ensuring strong eventual consistency without complex coordination",
  //   "state based crdts or cvrdts achieve strong eventual consistency by shipping the entire state merging involves a commutative associative idempotent join function ensuring convergence",
  //   "ot algorithms manage concurrent edits on shared documents common in collaborative editors they transform operations based on previously applied concurrent operations to ensure consistency",
  //   "gossip or epidemic protocols spread information through a network probabilistically nodes randomly exchange updates with peers ensuring eventual dissemination across the system efficiently",
  //   "anti entropy protocols periodically compare and reconcile replicas states in a distributed system they actively push or pull data to ensure convergence towards consistency",
  //   "consistent hashing minimizes key remapping when nodes are added or removed in a distributed hash table keys are assigned to the nearest clockwise node on a virtual ring",
  //   "rendezvous hashing highest random weight hashing allows clients to agree on server assignment for an object using a hashing function without central coordination or complex remapping",
  //   "chord is a peer to peer protocol implementing a distributed hash table it uses consistent hashing and finger tables for efficient o log n key lookups in a dynamic network",
  //   "distributed hash tables provide scalable key value storage they partition data across nodes using hashing offering decentralized lookup typically with logarithmic time complexity",
  //   "skip lists are probabilistic data structures providing efficient search like balanced trees skip graphs extend this concept for decentralized fault tolerant peer to peer network overlays",
  //   "pacelc extends cap in case of network partition systems trade availability vs consistency else normal operation they trade latency vs consistency it highlights inherent design tradeoffs",
  //   "harvest measures the fraction of data retrieved from an available system partition while yield measures the probability of completing a request they quantify degraded performance under failures",
  //   "base basically available soft state eventually consistent contrasts with acid it prioritizes availability over immediate consistency common in highly scalable web systems",
  //   "quorum systems ensure consistency by requiring read r and write w operations to overlap on n replicas r + w > n this allows tuning consistency levels",
  //   "pbs provides guarantees on data staleness within certain probabilistic bounds clients can estimate the maximum age of the data they might read from replicas",
  //   "shannon entropy quantifies the average information content or uncertainty of a data source it provides a theoretical lower bound for lossless data compression algorithms",
  //   "kolmogorov complexity measures the algorithmic complexity of an object as the length of the shortest program generating it it defines the ultimate limit of data compression",
  //   "huffman coding is a lossless compression algorithm creating variable length prefix codes more frequent symbols get shorter codes minimizing the average code length based on statistics",
  //   "lempel ziv algorithms like lz77 used in gzip and lzma in 7zip are dictionary based compressors they achieve compression by replacing repeated data sequences with references",
  //   "bloom filters probabilistically check set membership with potential false positives count min sketches estimate item frequencies in data streams both using compact space",
  //   "algorithms like prims or kruskals find a minimum spanning tree mst in a weighted grapha subset of edges connecting all vertices with minimum total edge weight",
  //   "in a directed graph strongly connected components sccs are maximal subgraphs where every vertex is reachable from every other vertex within that subgraph useful for dependency analysis",
  //   "betweenness centrality measures a nodes importance in a graph based on how often it lies on the shortest paths between other pairs of nodes high centrality indicates influence",
  //   "these algorithms divide a graphs vertices into balanced subsets while minimizing edge cuts between subsets used in parallel processing and distributed databases for load balancing",
  //   "pregel is a vertex centric model for large scale graph processing inspired by google computations occur in synchronous supersteps with vertices exchanging messages along edges",
  //   "semi joins reduce data transfer in distributed joins by sending only necessary joining column values bloom joins use bloom filters to filter rows unlikely to match",
  //   "sip enhances distributed query optimization by passing intermediate result properties like size or distinct values sideways between parallel plan branches to refine estimates",
  //   "data skew where some values are disproportionately frequent can bottleneck parallel query processing techniques involve repartitioning broadcasting small relations or specialized join algorithms",
  //   "dcbo extends traditional query optimization for distributed environments it considers network transfer costs data distribution and local processing costs to find efficient execution plans",
  //   "adaptive query processing techniques adjust query execution plans mid flight based on actual runtime statistics correcting initial misestimations and improving performance for complex queries",
  //   "mvcc allows readers to access older data versions while writers create new ones this avoids read write conflicts enabling high concurrency often providing snapshot isolation",
  //   "occ assumes conflicts are rare transactions execute on private copies then validate before committing if conflicts are detected via readwrite set checks transactions abort and retry",
  //   "these protocols ensure serializability by assigning timestamps to transactions conflicting operations are ordered based on these timestamps potentially causing aborts if order is violated",
  //   "predicate locking prevents phantom reads by locking data ranges based on logical predicates eg age > 30 rather than specific items ensuring serializability under complex conditions",
  //   "sgt detects concurrency conflicts by building a serialization graph where nodes are transactions and edges represent dependencies cycles in the graph indicate non serializable executions",
  //   "lsm trees optimize write performance by buffering writes in memory and merging them sequentially to disk reads may involve checking multiple sorted runs or levels",
  //   "wal ensures durability by writing changes to a sequential log file before applying them to the actual data pages this allows recovery after crashes by replaying the log",
  //   "checkpoints periodically save a consistent database state to disk reducing recovery time after a crash fuzzy checkpoints allow operations during the checkpoint process",
  //   "aries is a recovery algorithm standard known for its correctness and efficiency it uses wal repeating history during redo and undoing incomplete transactions during undo phases",
  //   "this approach recovers lost data or computation by tracking data dependencies lineage and recomputing only the necessary parts often used in big data systems like spark",
  //   "b+ trees are standard disk based index structures variants like prefix b trees or b* trees offer optimizations for specific workloads like string keys or higher node utilization",
  //   "r trees are spatial index structures using bounding boxes to index multi dimensional data r* trees are an optimized variant improving query performance through sophisticated node splitting",
  //   "concurrent skip lists provide efficient scalable search insertion and deletion in parallel environments they often use lock free techniques offering an alternative to balanced trees",
  //   "fractal trees or b trees are cache oblivious index structures aiming for better asymptotic io performance than b trees by buffering updates and applying them in batches",
  //   "prefix b trees optimize b+ trees for string keys by storing only discriminating prefixes in internal nodes reducing tree size and improving cache performance for string lookups",
  //   "phi accrual failure detectors output suspicion levels about node failures based on heartbeat arrival times adapting dynamically to network conditions unlike fixed timeout detectors",
  //   "swim is a gossip based protocol providing scalable robust group membership tracking it uses randomized probing and suspicion propagation for efficient failure detection",
  //   "virtual synchrony is a group communication model providing strong guarantees about message delivery order relative to membership changes simplifying fault tolerant application development",
  //   "in leaderless replication eg dynamo style any replica can accept writes consistency relies on mechanisms like read repair hinted handoff and quorum protocols",
  //   "atomic broadcast ensures messages are delivered reliably to all correct processes in the same total order even with failures crucial for state machine replication",
  //   "tla+ is a formal specification language used for designing modeling and verifying concurrent and distributed systems it helps catch high level design flaws early",
  //   "coq is an interactive theorem prover it allows expressing mathematical assertions and program properties and mechanically checking formal proofs of correctness ensuring high assurance",
  //   "linearizability is a strong consistency model operations appear to execute instantaneously and atomically at some point between their invocation and completion respecting real time order",
  //   "model checking automatically verifies if a system model satisfies a given formal specification often using temporal logic it explores the state space to find counterexamples",
  //   "bisimulation is a behavioral equivalence relation between state transition systems two systems are bisimilar if they can mimic each others transitions step by step",
  //   "inverted indices map terms to the documents containing them often storing positions posting lists this structure enables efficient full text search engines by quickly finding relevant documents",
  //   "lsi uses dimensionality reduction techniques like svd on term document matrices it aims to uncover latent semantic relationships improving retrieval by matching concepts not just keywords",
  //   "tf idf term frequency inverse document frequency measures word importance in a document relative to a corpus high scores indicate terms frequent locally but rare globally",
  //   "lsh hashes items such that similar items map to the same buckets with high probability it enables approximate nearest neighbor search efficiently in high dimensional spaces",
  //   "minhash efficiently estimates the jaccard similarity between sets eg document shingles it uses minimum hash values from random permutations to create compact set signatures",
  //   "homomorphic encryption allows computations like addition or multiplication to be performed directly on encrypted data without decrypting it first preserving privacy during processing",
  //   "zero knowledge proofs allow one party prover to convince another verifier that a statement is true without revealing any information beyond the statements truth itself",
  //   "threshold cryptography distributes cryptographic keys and operations like signing or decryption among multiple parties a threshold number of parties must cooperate to perform the operation",
  //   "differential privacy provides strong mathematically provable guarantees that query results on a dataset do not reveal sensitive information about any single individual within that dataset",
  //   "secure multi party computation smpc enables multiple parties to jointly compute a function over their private inputs without revealing those inputs to each other",
  //   "drf is a resource allocation algorithm for multi resource clusters cpu ram it aims for fairness by equalizing the dominant resource share allocated to each user",
  //   "bin packing algorithms try to fit items of various sizes into a minimum number of fixed capacity bins approximation algorithms find near optimal solutions efficiently for this np hard problem",
  //   "johnsons algorithm finds an optimal schedule minimizing the makespan total time for processing n jobs on two machines assuming each job has a fixed processing time on each",
  //   "work stealing deques double ended queues are used in parallel task scheduling idle processors steal tasks from the deques of busy processors to achieve load balancing",
  //   "these algorithms handle allocation problems where resources have multiple dimensions eg cpu memory bandwidth they often involve complex optimization techniques to satisfy diverse constraints",
  // ],
  prod: [
    "microservices architecture structures an application as a collection of loosely coupled independently deployable services enabling scalability and faster development cycles for complex systems",
    "api gateways act as a single entry point for all client requests routing them to appropriate backend services and handling cross cutting concerns like authentication rate limiting and logging",
    "idempotency ensures that making the same request multiple times produces the same result as making it once crucial for resilient distributed systems handling retries or network issues",
    "cqrs command query responsibility segregation separates read and write operations for a data store allowing for optimized data models and scaling strategies for each path",
    "event sourcing captures all changes to an application state as a sequence of events these events are persisted and can be replayed to reconstruct past states or build projections",
    "load balancing distributes incoming network traffic or computational workloads across multiple servers or resources to optimize resource use maximize throughput minimize response time and avoid overload",
    "sharding is a database partitioning technique that separates large databases into smaller faster more manageable parts called shards improving performance and scalability for high traffic applications",
    "CAP theorem states that a distributed data store can only provide two of three guarantees consistency availability and partition tolerance designers must choose which to prioritize",
    "service discovery mechanisms enable services in a distributed system to find and communicate with each other dynamically without hardcoded network locations essential in elastic environments",
    "circuit breaker patterns prevent an application from repeatedly trying to execute an operation that is likely to fail allowing it to fail fast and prevent cascading failures",
    "blue green deployment minimizes downtime by running two identical production environments blue and green traffic is switched to the new version green once it is verified",
    "canary releasing involves slowly rolling out a new software version to a small subset of users before a full deployment allowing for impact assessment and rollback if issues arise",
    "infrastructure as code iac manages and provisions computing infrastructure through machine readable definition files rather than physical hardware configuration or interactive configuration tools",
    "continuous integration ci is a devops practice where developers regularly merge their code changes into a central repository after which automated builds and tests are run",
    "continuous delivery cd extends continuous integration by automatically deploying all code changes to a testing andor production environment after the build stage has completed successfully",
    "docker containers package software with all its dependencies into a standardized unit for development shipment and deployment ensuring consistency across different environments",
    "kubernetes is an open source container orchestration platform for automating an applications deployment scaling and management allowing for resilient and efficient operation of containerized workloads",
    "object relational mapping orm is a programming technique for converting data between incompatible type systems using object oriented programming languages enabling easier database interaction",
    "nosql databases provide flexible schemas and often scale horizontally they are designed for specific data models and access patterns differing from traditional relational databases",
    "data lakes store vast amounts of raw data in its native format from various sources enabling flexible analytics and machine learning without predefined structures like data warehouses",
    "etl extract transform load processes are used to collect data from various sources transform it into a usable format and load it into a data warehouse or other system",
    "machine learning models are algorithms trained on data to make predictions or decisions without being explicitly programmed for each specific task learning patterns from examples",
    "deep learning a subset of machine learning utilizes artificial neural networks with multiple layers to model complex patterns in large datasets achieving state of the art results",
    "natural language processing nlp enables computers to understand interpret and generate human language bridging the gap between human communication and computational understanding",
    "computer vision is a field of artificial intelligence that trains computers to interpret and understand information from digital images or videos mimicking human visual capabilities",
    "blockchain technology provides a decentralized immutable ledger for recording transactions offering transparency and security without relying on a central authority",
    "zero trust is a security model assuming no implicit trust for any user or device requiring strict verification for every access request regardless of network location",
    "edge computing processes data closer to its source of generation rather than in a centralized cloud reducing latency bandwidth usage and improving responsiveness for iot and mobile applications",
    "a significant trend in modern software is the adoption of declarative apis where users specify the desired state and the system determines how to achieve it simplifying interactions",
    "the principle of least privilege dictates that entities should only have the minimum necessary access rights to perform their tasks enhancing system security by limiting potential damage",
    "gossip protocols enable decentralized information dissemination in large distributed systems where nodes periodically exchange state with random neighbors ensuring eventual consistency",
    "state machine replication is a fault tolerance technique where multiple replicas of a service execute the same sequence of operations ensuring consistent state even if some replicas fail",
    "the saga pattern manages distributed transactions using a sequence of local transactions each with a compensating transaction to revert changes if a step fails ensuring data consistency",
    "eventual consistency guarantees that if no new updates are made to a given data item all accesses to that item will eventually return the last updated value",
    "strong consistency ensures that all accesses to a data item return the most recently committed value reflecting an immediate and globally agreed upon state",
    "distributed hash tables dhts provide a decentralized lookup service similar to a hash table but data and responsibilities are distributed among many nodes in a peer to peer network",
    "acid properties atomicity consistency isolation durability are a set of guarantees ensuring that database transactions are processed reliably and maintain data integrity",
    "base properties basically available soft state eventual consistency offer a weaker consistency model than acid often used in nosql databases prioritizing availability and scalability",
    "data warehousing involves collecting storing and managing large volumes of historical data from various sources to support business intelligence reporting and analytical querying",
    "olap online analytical processing systems are optimized for complex queries and data analysis while oltp online transaction processing systems handle frequent short atomic transactions",
    "database indexing using structures like b trees or hash indexes speeds up data retrieval operations by providing efficient lookup paths at the cost of slower writes and storage space",
    "database replication copies data from a primary server to one or more secondary servers enhancing data availability fault tolerance and read scalability",
    "database normalization organizes data to reduce redundancy and improve data integrity by dividing larger tables into smaller related tables with defined relationships",
    "denormalization intentionally introduces redundancy into a database schema by merging tables to optimize read performance for specific queries sacrificing some write efficiency and data integrity",
    "graph databases use graph structures with nodes edges and properties to represent and store data ideal for managing highly connected datasets and complex relationships",
    "time series databases are optimized for storing and querying time stamped data sequences such as sensor readings server metrics or financial data supporting high ingest and analytical workloads",
    "domain driven design ddd is an approach to software development that emphasizes a deep understanding of the business domain modeling software closely to that domain",
    "bounded contexts in ddd define explicit boundaries within which a particular domain model is consistent and clear preventing model corruption in large complex systems",
    "event driven architecture eda promotes the production detection consumption of and reaction to events enabling loosely coupled asynchronous communication between system components",
    "serverless architecture allows developers to build and run applications without managing servers the cloud provider dynamically allocates and scales resources as needed",
    "message queues facilitate asynchronous communication between services by storing messages temporarily allowing senders and receivers to operate independently improving resilience and decoupling",
    "the publish subscribe pattern allows message senders publishers to categorize messages into topics without knowledge of which subscribers if any will receive them",
    "the repository pattern mediates between the domain and data mapping layers using a collection like interface for accessing domain objects abstracting data persistence",
    "dependency injection is a design pattern where an object receives its dependencies from an external source rather than creating them itself promoting loose coupling and testability",
    "solid principles single responsibility openclosed liskov substitution interface segregation dependency inversion guide object oriented design for maintainable and scalable software",
    "hexagonal architecture or ports and adapters isolates an applications core logic from external concerns like uis databases or apis by defining explicit interfaces ports",
    "site reliability engineering sre is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems focusing on reliability",
    "service level objectives slos are specific measurable targets for system reliability defining acceptable performance levels for service level indicators slis",
    "service level indicators slis are quantitative measures of service performance such as availability latency or error rate used to track and evaluate slos",
    "error budgets represent the acceptable level of unreliability for a service derived from slos allowing teams to balance feature velocity with reliability efforts",
    "gitops is a way of implementing continuous deployment for cloud native applications using git as the single source of truth for declarative infrastructure and applications",
    "chaos engineering is the discipline of experimenting on a distributed system in production in order to build confidence in the systems capability to withstand turbulent conditions",
    "observability in software systems refers to the ability to infer internal states from external outputs typically through metrics logs and distributed traces",
    "immutable infrastructure is a paradigm where servers or components are never modified after deployment new versions are deployed by replacing existing instances",
    "virtualization creates a virtual version of a computing resource like a server storage device or network allowing multiple operating systems to run on a single physical machine",
    "serverless functions also known as functions as a service faas allow running backend code without provisioning or managing servers executing in response to events",
    "a content delivery network cdn is a geographically distributed network of proxy servers and their data centers caching content close to end users to improve access speed",
    "elasticity is the ability of a system to automatically scale resources up or down based on demand while scalability refers to the systems capacity to handle increased load",
    "multi cloud strategy involves using services from multiple public cloud providers to avoid vendor lock in optimize costs or leverage best of breed services",
    "hybrid cloud combines a private cloud with one or more public cloud services allowing data and applications to be shared between them offering flexibility and control",
    "iaas infrastructure as a service provides virtualized computing resources over the internet users manage os applications while provider manages underlying infrastructure",
    "paas platform as a service offers a platform for developing running and managing applications without the complexity of building and maintaining infrastructure",
    "saas software as a service delivers software applications over the internet on a subscription basis eliminating the need for local installation and maintenance",
    "supervised learning is a machine learning task where models are trained on labeled data learning a mapping from input features to output labels",
    "unsupervised learning involves training models on unlabeled data to discover hidden patterns structures or relationships within the data itself such as clustering or dimensionality reduction",
    "reinforcement learning trains agents to make a sequence of decisions in an environment by maximizing a cumulative reward signal learning through trial and error",
    "artificial neural networks are computational models inspired by biological neural networks consisting of interconnected nodes or neurons that process information in layers",
    "convolutional neural networks cnns are specialized neural networks for processing grid like data such as images excelling at tasks like image recognition and object detection",
    "recurrent neural networks rnns are designed to process sequential data like text or time series using internal memory to capture temporal dependencies",
    "feature engineering is the process of selecting and transforming raw data into features that better represent the underlying problem to improve machine learning model performance",
    "overfitting occurs when a machine learning model learns the training data too well capturing noise and performing poorly on unseen data underfitting means the model is too simple",
    "gradient descent is an iterative optimization algorithm used to find the minimum of a function by repeatedly moving in the direction of the steepest descent of the loss function",
    "backpropagation is an algorithm used to train artificial neural networks by calculating the gradient of the loss function with respect to the network weights efficiently",
    "the tcpip model is a conceptual framework for internet protocols consisting of four layers application transport internet and link guiding network communication standards",
    "the osi open systems interconnection model is a seven layer conceptual framework that standardizes the functions of a telecommunication or computing system regardless of its underlying internal structure",
    "http hypertext transfer protocol is an application layer protocol for distributed hypermedia information systems forming the foundation of data communication for the world wide web",
    "dns domain name system is a hierarchical decentralized naming system that translates human readable domain names into machine readable ip addresses for locating network services",
    "firewalls are network security devices that monitor and filter incoming and outgoing network traffic based on predetermined security rules establishing a barrier between trusted and untrusted networks",
    "a virtual private network vpn extends a private network across a public network enabling users to send and receive data as if their devices were directly connected to the private network",
    "symmetric encryption uses the same secret key for both encrypting plaintext and decrypting ciphertext requiring secure key distribution between communicating parties",
    "asymmetric encryption or public key cryptography uses a pair of keys a public key for encryption and a private key for decryption enabling secure communication without pre shared secrets",
    "hashing algorithms transform input data of arbitrary size into a fixed size string of characters the hash value ensuring data integrity and enabling efficient data retrieval",
    "digital signatures use asymmetric cryptography to authenticate the origin and integrity of digital messages or documents providing non repudiation",
    "oauth 2.0 is an authorization framework enabling third party applications to access user resources on a web service without exposing user credentials typically using access tokens",
    "openid connect oidc is an identity layer built on top of oauth 2.0 allowing clients to verify user identity based on authentication performed by an authorization server",
    "a man in the middle mitm attack occurs when an attacker secretly intercepts and relays communication between two parties who believe they are directly communicating with each other",
    "sql injection is a code injection technique that exploits security vulnerabilities in an applications database layer by inserting malicious sql statements into entry fields",
    "cross site scripting xss is a web security vulnerability allowing attackers to inject malicious scripts into web pages viewed by other users potentially stealing session tokens or defacing sites",
    "defense in depth is a security strategy that employs multiple layers of protective measures so that if one layer fails another layer is already in place to thwart an attack",
    "concurrency allows multiple tasks to make progress over overlapping time periods while parallelism enables multiple tasks to execute simultaneously typically on multi core processors",
    "multithreading enables a single process to manage multiple threads of execution concurrently allowing for improved responsiveness and resource utilization in applications",
    "process synchronization mechanisms like mutexes and semaphores coordinate access to shared resources among concurrent processes or threads preventing race conditions and ensuring data consistency",
    "memory management involves allocating and deallocating memory resources for programs with techniques like garbage collection automatically reclaiming memory no longer in use",
    "functional programming is a paradigm treating computation as the evaluation of mathematical functions emphasizing immutability pure functions and avoiding side effects",
    "object oriented programming oop organizes software around objects which encapsulate data and behavior promoting concepts like inheritance polymorphism and encapsulation",
    "hash tables are data structures that implement an associative array abstract data type mapping keys to values using a hash function for efficient lookups insertions and deletions",
    "trees are hierarchical data structures consisting of nodes connected by edges commonly used for representing hierarchical data searching and sorting like binary search trees",
    "graphs are data structures composed of vertices nodes and edges representing relationships between them used to model networks social connections and various complex systems",
    "big o notation describes the limiting behavior of a functions growth rate providing an asymptotic upper bound on algorithm complexity in terms of input size",
    "webassembly wasm is a binary instruction format for a stack based virtual machine designed as a portable compilation target for high performance applications on the web",
    "progressive web apps pwas are web applications that use modern web capabilities to deliver an app like experience to users including offline support and push notifications",
    "rest representational state transfer is an architectural style for designing networked applications emphasizing stateless client server communication and standard http methods",
    "graphql is a query language for apis and a server side runtime for executing those queries providing clients with exactly the data they request and nothing more",
    "digital twins are virtual representations of physical objects processes or systems used for simulation monitoring analysis and optimization in real time",
    "federated learning is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples without exchanging them",
    "explainable ai xai aims to create ai systems whose decisions and predictions can be understood by humans fostering trust transparency and accountability in ai applications",
    "quantum computing leverages principles of quantum mechanics to perform complex calculations exponentially faster than classical computers for specific types of problems",
    "homomorphic encryption allows computations to be performed on encrypted data without decrypting it first enabling privacy preserving data processing and analysis",
    "zero knowledge proofs zkps enable one party to prove to another that a statement is true without revealing any information beyond the validity of the statement itself",
    "geospatial data refers to information that has a geographic component describing objects events or other features with a location on or near the surface of the earth",
    "data governance establishes policies standards and processes for managing an organizations data assets ensuring data quality security usability and compliance",
    "a data mesh is a decentralized sociotechnical approach to data architecture that organizes data by domain treating data as a product and enabling self service data infrastructure",
    "a feature store is a central repository for storing documenting and managing machine learning features enabling their reuse across multiple models and projects",
    "mpp massively parallel processing architectures distribute data and computation across many independent nodes or processors working in parallel to execute complex queries quickly",
    "stream processing involves analyzing and acting on data in real time as it is generated or received enabling immediate insights and responses to events",
    "batch processing processes large volumes of data collected over a period by executing jobs in batches suitable for tasks not requiring real time results",
    "throttling is a control mechanism used to limit the rate at which requests are processed by a system preventing overload and ensuring fair usage among clients",
    "deadlock is a situation where two or more processes are blocked indefinitely each waiting for a resource held by another process in the set preventing progress",
    "data lineage tracks the origin movement transformations and usage of data throughout its lifecycle providing visibility and auditability for data governance and troubleshooting",
    "a bloom filter is a space efficient probabilistic data structure used to test whether an element is a member of a set allowing for false positives but no false negatives",
    "a reverse proxy sits in front of web servers forwarding client requests to those servers typically used for load balancing caching ssl termination and security",
    "consistent hashing is a distributed hashing scheme that minimizes data remapping when nodes are added or removed from a cluster crucial for scalable distributed caches and storage",
    "sidecar pattern deploys auxiliary components alongside a main application container to provide supporting features like logging monitoring or proxying enhancing modularity",
    "webhooks are automated messages sent from apps when something happens they have a message payload which is sent to a unique url a simple event notification via http post",
    "a content security policy csp is a security standard that helps prevent cross site scripting xss and other code injection attacks by specifying allowed content sources",
  ],
};
